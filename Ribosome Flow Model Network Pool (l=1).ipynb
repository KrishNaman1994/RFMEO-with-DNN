{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626b4763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "rc('axes', linewidth=1.2)\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5641c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setting up the parameters of the RFMEONP\n",
    "\"\"\"\n",
    "sites = [3,2]\n",
    "lamda1 = [1+i/4 for i in range(1,5)]\n",
    "lamda2= [1.0+i/3 for i in range(1,4)]\n",
    "lamda = [lamda1, lamda2]\n",
    "p = sum(sites)+1\n",
    "poolsize = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ea98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generating the Training and Test Sets\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Training Set Generation\n",
    "\"\"\"\n",
    "h = 0.1\n",
    "t = np.arange(0, 1+h, h)\n",
    "t = t.reshape((1,len(t)))\n",
    "ntrain = t.shape[1]\n",
    "scale = 10000\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Testing Set Generation\n",
    "\"\"\"\n",
    "h = 0.0001\n",
    "t0 = np.arange(0, .9+h,h)\n",
    "t0 = t0.reshape((1,len(t0)))\n",
    "n0 = t0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee5fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(Z):\n",
    "    return Z\n",
    "\n",
    "def Gprime(Z):\n",
    "    return 1.0\n",
    "\n",
    "def funct(y, lamda, sites, H):\n",
    "    f = y.copy()\n",
    "    n = sum(sites)\n",
    "    sites0 = 0\n",
    "    sitesn = 0\n",
    "    ii = 0\n",
    "    z = 2*H*y[n]\n",
    "    for i in range(len(sites)):\n",
    "        k = sum(sites[:i])\n",
    "        p = sites[i]\n",
    "        ii = 0\n",
    "        f[k] = lamda[i][ii]*G(z)*(1-y[k])-lamda[i][ii+1]*y[k]*(1-y[k+1])\n",
    "        sites0 += lamda[i][ii]*G(z)*(1-y[k])\n",
    "        ii += 1\n",
    "        for j in range(k+1, k+p-1):\n",
    "            f[j] = lamda[i][ii]*y[j-1]*(1-y[j])-lamda[i][ii+1]*y[j]*(1-y[j+1])\n",
    "            ii += 1\n",
    "        f[k+p-1] = lamda[i][ii]*y[k+p-2]*(1-y[k+p-1])-lamda[i][ii+1]*y[k+p-1]\n",
    "        sitesn += lamda[i][ii+1]*y[k+p-1]\n",
    "    f[n] = 1/(2*H)*(sitesn-sites0)\n",
    "    return scale*f\n",
    "\n",
    "def init_weights_biases(initialiser, N0, N1):\n",
    "    if initialiser.upper() == 'NORMAL':\n",
    "        return [np.random.normal(0,np.sqrt(2/N1),(N1,N0)), np.random.normal(0, np.sqrt(2/N1), (N1,1))]\n",
    "    if initialiser.upper() == 'UNIFORM':\n",
    "        return [np.random.uniform(0,0.05,(N1,N0)),np.random.uniform(0,0.05,(N1,1))]\n",
    "    if initialiser.upper() == 'XAVIER':\n",
    "        return [np.random.uniform(0, 1/np.sqrt(N1), (N1,N0)), np.random.normal(0, 1/np.sqrt(N1), (N1,1))]\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def add_layer(input_shape, hidden_units , activation = 'sigmoid', initialiser = 'normal'): \n",
    "    weights_biases = init_weights_biases(initialiser, input_shape, hidden_units)\n",
    "    NA.append(activation)\n",
    "    NW.append(weights_biases[0])\n",
    "    NB.append(weights_biases[1])\n",
    "    return None\n",
    "\n",
    "def estimated_derivative(A, Z, NA, NW, NB, t):\n",
    "    dydt = A[2]+t*(NW[1]@((activation_derivative(Z[0], NA[0])*(NW[0]@np.ones_like(t)))))\n",
    "    return dydt\n",
    "\n",
    "def activation_function(x, string, alpha = 0.01):\n",
    "    if string.upper() == 'SIGMOID':\n",
    "        return (1/(1+np.exp(-x)))\n",
    "    if string.upper() == 'BPS':\n",
    "        return  1-2*(1/(1+np.exp(-x)))\n",
    "    if string.upper() == 'TRIG' or string.upper() == 'TRIGNOMETRIC':\n",
    "        return np.cos(x)\n",
    "    if string.upper() == 'CUSTOM':\n",
    "        return 0.9*np.tanh(x)-0.5*(1/(1+np.exp(-x)))\n",
    "    if string.upper() == 'TANH':\n",
    "        return np.tanh(x)\n",
    "    if string.upper() == 'RELU':\n",
    "        return (x+np.abs(x))\n",
    "    if string.upper() == 'LEAKYRELU' or string.upper() == 'LR':\n",
    "        return (x+alpha*x+np.abs(x-alpha*x))/2\n",
    "    if string.upper() == 'LINEAR':\n",
    "        return x\n",
    "    if string.upper() == 'EXPONENTIAL' or string.upper() == 'EXP':\n",
    "        return np.exp(x)\n",
    "    if string.upper() == 'ELU':\n",
    "        x[x<0] = 0.01*(np.exp(x[x<0])-1)\n",
    "        return x\n",
    "    if string.upper() == 'EXP':\n",
    "        return np.exp(-x)\n",
    "    return None\n",
    "    \n",
    "def activation_derivative(x, string, alpha = 0.01):\n",
    "    if string.upper() == 'SIGMOID':\n",
    "        return (1/(1+np.exp(-x)))*(1-(1/(1+np.exp(-x))))\n",
    "    if string.upper() == 'BPS':\n",
    "        return -2*(1/(1+np.exp(-x)))*(1-(1/(1+np.exp(-x))))\n",
    "    if string.upper() == 'TRIG' or string.upper() == 'TRIGNOMETRIC':\n",
    "        return -np.sin(x)\n",
    "    if string.upper() == 'TANH':\n",
    "        return (1-np.tanh(x)**2)\n",
    "    if string.upper() == \"RELU\":\n",
    "        x[x<0] = 0\n",
    "        x[x>=0] = 1\n",
    "        return x\n",
    "    if string.upper() == 'CUSTOM':\n",
    "        return 0.9*(1-np.tanh(2*x)**2)-0.5*(1/(1+np.exp(-x)))*(1-(1/(1+np.exp(-x))))\n",
    "    if string.upper() == 'LEAKYRELU' or string.upper() == 'LR':\n",
    "        dx = np.ones(x.shape)\n",
    "        dx[x < 0] = alpha\n",
    "        x = dx.copy()\n",
    "        return x\n",
    "    if string.upper() == 'LINEAR':\n",
    "        x = 1\n",
    "        return x\n",
    "    if string.upper() == 'EXPONENTIAL' or string.upper() == 'EXP':\n",
    "        return np.exp(x)\n",
    "    if string.upper() == 'ELU':\n",
    "        x[x>=0] = 1\n",
    "        x[x<0] = 0.01*(np.exp(x[x<0]))\n",
    "        return x\n",
    "    if string.upper() == 'EXP':\n",
    "        return -np.exp(-x)\n",
    "    return None\n",
    "    \n",
    "def forward_propagation(X,NA,NW,NB):\n",
    "    A = [X]\n",
    "    Z = []\n",
    "    for i in range(len(NA)):\n",
    "        Zstar = (NW[i]@A[i]+NB[i])\n",
    "        Astar = activation_function(Zstar.astype(float), NA[i])\n",
    "        Z.append(Zstar.astype(float))\n",
    "        A.append(Astar)\n",
    "    return([Z,A])\n",
    "\n",
    "def backward_propagation(NA, NW, Z, A, dZ, dW, dB, y, f, H, sites, lamda, alpha):\n",
    "    Adot = (np.gradient(A[L])[0]/scale)-10**-10\n",
    "    Adotdot = (np.gradient(Adot)[0]/scale)\n",
    "    dAL = [0 for i in range(sum(sites)+1)]\n",
    "    n = sum(sites)\n",
    "    Y = 2*H*y[n]\n",
    "    total = y[n]\n",
    "    penalty = alpha\n",
    "    penalty2 = 1\n",
    "    for j in range(n):\n",
    "        total += y[j]/(2*H)\n",
    "    for i in range(len(sites)):\n",
    "        k = sum(sites[:i])\n",
    "        p = sites[i]\n",
    "        ii = 0\n",
    "        df0a0 = -lamda[i][ii]*G(Y)*t-lamda[i][ii+1]*t*(1-y[k+1])\n",
    "        df1a0 = lamda[i][ii+1]*t*(1-y[k+1])\n",
    "        dfza0 = +1/(2*H)*lamda[i][0]*G(Y)*t\n",
    "        dAL[k] = (A[L][k]+t*Adot[k]-f[k])*(1+t*Adotdot[k]/Adot[k]-df0a0)-(A[L][k+1]+t*Adot[k+1]-f[k+1])*df1a0-(A[L][n]+t*Adot[n]-f[n])*dfza0-penalty*(0.5-total)*t\n",
    "        ii += 1\n",
    "        \n",
    "        for j in range(k+1, k+p-1):\n",
    "            df0a1 = lamda[i][ii]*y[j-1]*t\n",
    "            df1a1 = -lamda[i][ii]*y[j-1]*t-lamda[i][ii+1]*t*(1-y[j+1])\n",
    "            df2a1 = lamda[i][ii+1]*t*(1-y[j+1])\n",
    "            dAL[j] = -(A[L][j-1]+t*Adot[j-1]-f[j-1])*df0a1+(A[L][j]+t*Adot[j]-f[j])*(1+t*Adotdot[j]/Adot[j]-df1a1)-(A[L][j+1]+t*Adot[j+1]-f[j+1])*df2a1-penalty*(0.5-total)*t\n",
    "            ii += 1\n",
    "\n",
    "        df4a5 = lamda[i][ii]*y[ii-1]*t\n",
    "        df5a5 = -lamda[i][ii]*y[ii-1]*t-lamda[i][ii+1]*t\n",
    "        dfza5 = 1/(2*H)*lamda[i][ii+1]*t\n",
    "        dAL[k+p-1] = penalty2*(-(A[L][k+p-2]+t*Adot[k+p-2]-f[k+p-2])*df4a5+(A[L][k+p-1]+t*Adot[k+p-1]-f[k+p-1])*(1+t*Adotdot[k+p-1]/Adot[k+p-1]-df5a5)-(A[L][n]+t*Adot[n]-f[n])*dfza5)-penalty*(0.5-total)*t\n",
    "    s = 0\n",
    "    for i in range(len(sites)):\n",
    "        k = sum(sites[:i])\n",
    "        df0az = lamda[i][0]*(1-y[k])\n",
    "        s += -df0az\n",
    "        dAL[n] += -penalty2*(A[L][k]+t*Adot[k]-f[k])*Gprime(Y)*t*df0az\n",
    "    dAL[n] += penalty2*((A[L][n]+t*Adot[n]-f[n])*(1+t*Adotdot[n]/Adot[n]-Gprime(Y)*2*H*t*s))-2*H*penalty*(0.5-total)*t\n",
    "    \n",
    "    dtAL = np.array(dAL).reshape((sum(sites)+1,ntrain))/ntrain\n",
    "    for i in range(L-1,-1,-1):    \n",
    "        dZ[i] = dtAL*activation_derivative(Z[i],NA[i])\n",
    "        dW[i] = (dZ[i]@A[i].T)/ntrain\n",
    "        dB[i] = np.sum(dZ[i], axis = 1, keepdims = True)/ntrain\n",
    "        dtAL = (NW[i].T@dZ[i])/ntrain\n",
    "    return [dZ, dW, dB]    \n",
    "\n",
    "def rmsprop(NW, NB, dW, dB, SW, SB, epsilon, lr, beta):\n",
    "    for i in range(L):\n",
    "        SW[i] = (beta*SW[i]+(1-beta)*dW[i]**2)\n",
    "        SB[i] = (beta*SB[i]+(1-beta)*dB[i]**2)\n",
    "        NW[i] = NW[i]-lr*dW[i]/(SW[i]**0.5+epsilon)\n",
    "        NB[i] = NB[i]-lr*dB[i]/(SB[i]**0.5+epsilon)\n",
    "    return [NW, NB, SW, SB]\n",
    "\n",
    "def adam(i, NW, NB, dW, dB, VW, VB, SW, SB, epsilon, lr, momentum, beta):\n",
    "    VWhat = VW.copy()\n",
    "    VBhat = VB.copy()\n",
    "    SWhat = SW.copy()\n",
    "    SBhat = SB.copy()\n",
    "    for j in range(L):\n",
    "        VW[j] = momentum*VW[j]+(1-momentum)*dW[j]\n",
    "        VB[j] = momentum*VB[j]+(1-momentum)*dB[j]\n",
    "        SW[j] = beta*SW[j]+(1-beta)*(dW[j]**2)\n",
    "        SB[j] = beta*SB[j]+(1-beta)*(dB[j]**2)\n",
    "        VWhat[j] = VW[j]/(1-momentum**i)\n",
    "        VBhat[j] = VB[j]/(1-momentum**i)\n",
    "        SWhat[j] = SW[j]/(1-beta**i)\n",
    "        SBhat[j] = SB[j]/(1-beta**i)\n",
    "        NW[j] = NW[j]-lr*(VWhat[j]/np.sqrt(SWhat[j]+epsilon))\n",
    "        NB[j] = NB[j]-lr*(VBhat[j]/np.sqrt(SBhat[j]+epsilon))\n",
    "    return [NW, NB, VW, VB, SW, SB]\n",
    "\n",
    "def train_model(X, mean, epochs, NA, NW, NB, init, optimiser = 'sgd', learning_rate = 0.001, \n",
    "                momentum = 0.9, epsilon = 10**-8, beta = 0.999, poolsize = 1, alpha = 1):\n",
    "    [dZ, dW, dB] = [[0 for i in range(L)],[0 for i in range(L)],[0 for i in range(L)]]\n",
    "    VW = [np.zeros(NW[i].shape) for i in range(L)]\n",
    "    VB = [np.zeros(NB[i].shape) for i in range(L)]\n",
    "    SW = [np.zeros(NW[i].shape) for i in range(L)]\n",
    "    SB = [np.zeros(NB[i].shape) for i in range(L)]\n",
    "    for i in range(epochs):\n",
    "        [Z, A] = forward_propagation(X, NA, NW, NB)\n",
    "        yhat = init+t*(A[L])\n",
    "        ydot = estimated_derivative(A, Z, NA, NW, NB, t)\n",
    "        f = funct(yhat, lamda, sites, poolsize)\n",
    "        [dZ, dW, dB] = backward_propagation(NA, NW, Z, A, dZ, dW, dB, yhat, f, poolsize, sites, lamda, alpha)\n",
    "        [NW, NB, VW, VB, SW, SB] =  adam(i+1, NW, NB, dW, dB, VW, VB, SW, SB, epsilon, learning_rate, momentum, beta)\n",
    "#         learning_rate = learning_rate*0.999**500\n",
    "#         [NW, NB, SW, SB] = rmsprop(NW, NB, dW, dB, SW, SB, epsilon, learning_rate, beta)\n",
    "        if (i/epochs)*100 in range(100):\n",
    "            print('█', end = '')\n",
    "#     print('\\n')\n",
    "    return [NW, NB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78a802ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.████████████████████████████████████████████████████████████████████████████████████████████2.█████"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m add_layer(input_shape \u001b[38;5;241m=\u001b[39m p, hidden_units \u001b[38;5;241m=\u001b[39m p, activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbps\u001b[39m\u001b[38;5;124m'\u001b[39m, initialiser \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxavier\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mTraining the CDNN on the train set\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m   \n\u001b[1;32m---> 22\u001b[0m [NW, NB] \u001b[38;5;241m=\u001b[39m train_model(t, np\u001b[38;5;241m.\u001b[39mmean(init\u001b[38;5;241m.\u001b[39mT), \u001b[38;5;241m15000\u001b[39m, NA, NW, NB, init, optimiser \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     23\u001b[0m                                  learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, momentum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m, beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m, poolsize \u001b[38;5;241m=\u001b[39m poolsize, alpha \u001b[38;5;241m=\u001b[39m alpha)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03mObtaining the solutions on the test set\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m \n\u001b[0;32m     27\u001b[0m [Z,A] \u001b[38;5;241m=\u001b[39m forward_propagation(t0, NA, NW, NB)\n",
      "Cell \u001b[1;32mIn[7], line 205\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X, mean, epochs, NA, NW, NB, init, optimiser, learning_rate, momentum, epsilon, beta, poolsize, alpha)\u001b[0m\n\u001b[0;32m    203\u001b[0m         ydot \u001b[38;5;241m=\u001b[39m estimated_derivative(A, Z, NA, NW, NB, t)\n\u001b[0;32m    204\u001b[0m         f \u001b[38;5;241m=\u001b[39m funct(yhat, lamda, sites, poolsize)\n\u001b[1;32m--> 205\u001b[0m         [dZ, dW, dB] \u001b[38;5;241m=\u001b[39m backward_propagation(NA, NW, Z, A, dZ, dW, dB, yhat, f, poolsize, sites, lamda, alpha)\n\u001b[0;32m    206\u001b[0m         [NW, NB, VW, VB, SW, SB] \u001b[38;5;241m=\u001b[39m  adam(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, NW, NB, dW, dB, VW, VB, SW, SB, epsilon, learning_rate, momentum, beta)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m#         learning_rate = learning_rate*0.999**500\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m#         [NW, NB, SW, SB] = rmsprop(NW, NB, dW, dB, SW, SB, epsilon, learning_rate, beta)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 120\u001b[0m, in \u001b[0;36mbackward_propagation\u001b[1;34m(NA, NW, Z, A, dZ, dW, dB, y, f, H, sites, lamda, alpha)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_propagation\u001b[39m(NA, NW, Z, A, dZ, dW, dB, y, f, H, sites, lamda, alpha):\n\u001b[1;32m--> 120\u001b[0m     Adot \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mgradient(A[L])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mscale)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m    121\u001b[0m     Adotdot \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mgradient(Adot)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mscale)\n\u001b[0;32m    122\u001b[0m     dAL \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28msum\u001b[39m(sites)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:1266\u001b[0m, in \u001b[0;36mgradient\u001b[1;34m(f, axis, edge_order, *varargs)\u001b[0m\n\u001b[0;32m   1264\u001b[0m     dx_n \u001b[38;5;241m=\u001b[39m ax_dx \u001b[38;5;28;01mif\u001b[39;00m uniform_spacing \u001b[38;5;28;01melse\u001b[39;00m ax_dx[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;66;03m# 1D equivalent -- out[-1] = (f[-1] - f[-2]) / (x[-1] - x[-2])\u001b[39;00m\n\u001b[1;32m-> 1266\u001b[0m     out[\u001b[38;5;28mtuple\u001b[39m(slice1)] \u001b[38;5;241m=\u001b[39m (f[\u001b[38;5;28mtuple\u001b[39m(slice2)] \u001b[38;5;241m-\u001b[39m f[\u001b[38;5;28mtuple\u001b[39m(slice3)]) \u001b[38;5;241m/\u001b[39m dx_n\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;66;03m# Numerical differentiation: 2nd order edges\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1270\u001b[0m     slice1[axis] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p = sum(sites)+1\n",
    "alpha = 1000\n",
    "errors = []\n",
    "rn = [[] for j in range(p)]\n",
    "zs = []\n",
    "L = 3\n",
    "k = 10\n",
    "for ii in range(k):\n",
    "    print(ii+1, end = '.')\n",
    "    b = min(1,poolsize*(1/sum(sites)))\n",
    "    init = np.random.uniform(0, b, (p,1))\n",
    "    #         init = np.zeros((p,1))\n",
    "    init[-1] = 0.5-1/(2*poolsize)*(sum(init[:-1]))\n",
    "    [NA, NW, NB] = [[],[],[]]\n",
    "    add_layer(input_shape = 1, hidden_units = p, activation = 'tanh', initialiser = 'xavier')\n",
    "    for i in range(L-2):\n",
    "        add_layer(input_shape = p, hidden_units = p, activation = 'sigmoid', initialiser = 'xavier')\n",
    "    add_layer(input_shape = p, hidden_units = p, activation = 'bps', initialiser = 'xavier')\n",
    "    \"\"\"\n",
    "    Training the CDNN on the train set\n",
    "    \"\"\"   \n",
    "    [NW, NB] = train_model(t, np.mean(init.T), 15000, NA, NW, NB, init, optimiser = 'rmsprop',\n",
    "                                     learning_rate = 10**-2, momentum = 0.9, beta = 0.999, poolsize = poolsize, alpha = alpha)\n",
    "    \"\"\"\n",
    "    Obtaining the solutions on the test set\n",
    "    \"\"\" \n",
    "    [Z,A] = forward_propagation(t0, NA, NW, NB)\n",
    "    v_hat = (init+t0*(A[L]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
